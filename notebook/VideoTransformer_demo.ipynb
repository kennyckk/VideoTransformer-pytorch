{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoTransformer\n",
    "\n",
    "TimeSformer(https://arxiv.org/abs/2102.05095), ViViT(https://arxiv.org/abs/2103.15691)\n",
    "\n",
    "Welcome to the demo notebook for VideoTransformer. We'll showcase the prediction result by the above pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This section contains initial setup. Run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gv1qdarnNNVE",
    "outputId": "eaac44da-a976-4fda-b323-03dfa6790e21",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!pip3 install --user torch\n",
    "!pip3 install --user torchvision\n",
    "!pip3 install --user matplotlib\n",
    "!pip3 install --user decord\n",
    "!pip3 install --user einops\n",
    "!pip3 install --user scikit-image\n",
    "!pip3 install --user pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBBG_T32pzPH",
    "outputId": "76921b65-9e35-4260-fa0a-bbb484ac285c",
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenny\\Desktop\\Poly\\22_23_sem2\\project\\proposed\\vivit_testing\\VideoTransformer-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\kenny\\Desktop\\Poly\\22_23_sem2\\project\\proposed\\vivit_testing\\VideoTransformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data_transform as T\n",
    "from dataset import DecordInit, load_annotation_data\n",
    "from transformer import PatchEmbed, TransformerContainer, ClassificationHead\n",
    "from video_transformer import ViViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Please firstly dowload the weights and move to the current path `./VideoTransformer-pytorch/`\n",
    "1. TimeSformer-B pre-trained on K400 https://drive.google.com/file/d/1jLkS24jkpmakPi3e5J8KH3FOPv370zvo/view?usp=sharing\n",
    "2. ViViT-B pre-trained on K400 from https://drive.google.com/file/d/1-JVhSN3QHKUOLkXLWXWn5drdvKn0gPll/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yLko87R2_m4",
    "pycharm": {}
   },
   "source": [
    "## Video Transformer Model\n",
    "\n",
    "We here load the pretrained weights of the transformer model TimeSformer-B or ViViT-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "â€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_state_dict(state_dict):\n",
    "\tfor old_key in list(state_dict.keys()):\n",
    "\t\tif old_key.startswith('model'):\n",
    "\t\t\tnew_key = old_key[6:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)\n",
    "\t\telse:\n",
    "\t\t\tnew_key = old_key[9:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_from_pretrain_(module, pretrained, init_module):\n",
    "    if torch.cuda.is_available():\n",
    "        state_dict = torch.load(pretrained)\n",
    "    else:\n",
    "        state_dict = torch.load(pretrained, map_location=torch.device('cpu'))\n",
    "    if init_module == 'transformer':\n",
    "        replace_state_dict(state_dict)\n",
    "    elif init_module == 'cls_head':\n",
    "        replace_state_dict(state_dict)\n",
    "    else:\n",
    "        raise TypeError(f'pretrained weights do not include the {init_module} module')\n",
    "    msg = module.load_state_dict(state_dict, strict=False)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "u5J7lGPJ2bLJ",
    "outputId": "41c0568b-082f-4609-8a5e-9ff4b80ffd92",
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['cls_head.weight', 'cls_head.bias'])\n",
      "load model finished, the missing key of cls is:[]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "num_frames = 8\n",
    "frame_interval = 32\n",
    "num_class = 400\n",
    "arch = 'vivit' # turn to vivit for initializing vivit model\n",
    "\n",
    "if arch == 'timesformer':\n",
    "    pretrain_pth = './timesformer_k400.pth'\n",
    "    model = TimeSformer(num_frames=num_frames,\n",
    "                        img_size=224,\n",
    "                        patch_size=16,\n",
    "                        embed_dims=768,\n",
    "                        in_channels=3,\n",
    "                        attention_type='divided_space_time',\n",
    "                        return_cls_token=True)\n",
    "elif arch == 'vivit':\n",
    "    pretrain_pth = './vivit_model.pth'\n",
    "    num_frames = num_frames * 2\n",
    "    frame_interval = frame_interval // 2\n",
    "    model = ViViT(num_frames=num_frames,\n",
    "                  img_size=224,\n",
    "                  patch_size=16,\n",
    "                  embed_dims=768,\n",
    "                  in_channels=3,\n",
    "                  attention_type='fact_encoder',\n",
    "                  return_cls_token=True,\n",
    "                    weights_from=\"kinetics\",\n",
    "                     pretrain_pth=pretrain_pth)\n",
    "else:\n",
    "    raise TypeError(f'not supported arch type {arch}, chosen in (timesformer, vivit)')\n",
    "\n",
    "cls_head = ClassificationHead(num_classes=num_class, in_channels=768)\n",
    "#msg_trans = init_from_pretrain_(model, pretrain_pth, init_module='transformer')\n",
    "msg_cls = init_from_pretrain_(cls_head, pretrain_pth, init_module='cls_head')\n",
    "\n",
    "model.eval()\n",
    "cls_head.eval()\n",
    "model = model.to(device)\n",
    "cls_head = cls_head.to(device)\n",
    "print(f'load model finished, the missing key of cls is:{msg_cls[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "\n",
    "Here we show the video demo and transform the video input for the model processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"480\" height=\"480\" src=\"./demo/YABnJL_bDzw.mp4\">animation</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "video_path = './demo/YABnJL_bDzw.mp4'\n",
    "html_str = '''\n",
    "<video controls width=\\\"480\\\" height=\\\"480\\\" src=\\\"{}\\\">animation</video>\n",
    "'''.format(video_path)\n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 256, 454, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 3, 224, 224])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data preprocess\n",
    "mean, std = (0.45, 0.45, 0.45), (0.225, 0.225, 0.225)\n",
    "data_transform = T.Compose([\n",
    "        T.Resize(scale_range=(-1, 256)),\n",
    "        T.ThreeCrop(size=224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std)\n",
    "        ])\n",
    "temporal_sample = T.TemporalRandomCrop(num_frames*frame_interval)\n",
    "\n",
    "# Sampling video frames\n",
    "video_decoder = DecordInit()\n",
    "v_reader = video_decoder(video_path)\n",
    "total_frames = len(v_reader)\n",
    "start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
    "if end_frame_ind-start_frame_ind < num_frames:\n",
    "    raise ValueError(f'the total frames of the video {video_path} is less than {num_frames}')\n",
    "frame_indice = np.linspace(0, end_frame_ind-start_frame_ind-1, num_frames, dtype=int)\n",
    "video = v_reader.get_batch(frame_indice).asnumpy()\n",
    "del v_reader\n",
    "\n",
    "display(video.shape)\n",
    "video = torch.from_numpy(video).permute(0,3,1,2) # Video transform: T C H W\n",
    "data_transform.randomize_parameters()\n",
    "video = data_transform(video)\n",
    "display(video.shape)\n",
    "video = video.to(device)\n",
    "\n",
    "#the 3 additional \"batch\" are from the ThreeCrop Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Classification\n",
    "\n",
    "Here we use the pre-trained video transformer to classify the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of ouptut: torch.Size([400]), and the prediction is: laughing\n"
     ]
    }
   ],
   "source": [
    "# Predict class label\n",
    "with torch.no_grad():\n",
    "    logits = model(video)\n",
    "    display(logits.shape)\n",
    "    output = cls_head(logits)\n",
    "    display(output.shape)\n",
    "    output = output.view(3, 400).mean(0)\n",
    "    \n",
    "    cls_pred = output.argmax().item()\n",
    "\n",
    "class_map = './k400_classmap.json'\n",
    "class_map = load_annotation_data(class_map)\n",
    "for key, value in class_map.items():\n",
    "    if int(value) == int(cls_pred):\n",
    "        print(f'the shape of ouptut: {output.shape}, and the prediction is: {key}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "iBOT_demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "vivit2",
   "language": "python",
   "name": "vivit2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10cb415f29954cb0a511e263fcb6c69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecfd9523d50f4f7f87083460dc0e3dd7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6d68baa05ccc427890286e0e74452155",
      "value": " 327M/327M [00:09&lt;00:00, 36.6MB/s]"
     }
    },
    "3cb57e69cf054551b1978ddd6be3553b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bbd072a4a424ef88d5354793942a84b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6668b553344743e2a1450251c5a237e2",
      "value": "100%"
     }
    },
    "557451188ee74f3698a8b358ee70d29d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af53b2f6e65b43fb8b3df5c656b40c92",
      "max": 343279349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e962b8959ca4e90b496ab36d764f073",
      "value": 343279349
     }
    },
    "5bbd072a4a424ef88d5354793942a84b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6668b553344743e2a1450251c5a237e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d68baa05ccc427890286e0e74452155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8902dbcd84b4439a987e908be8c8e19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3cb57e69cf054551b1978ddd6be3553b",
       "IPY_MODEL_557451188ee74f3698a8b358ee70d29d",
       "IPY_MODEL_10cb415f29954cb0a511e263fcb6c69f"
      ],
      "layout": "IPY_MODEL_95c73030d06d4d578252f47989c208fc"
     }
    },
    "95c73030d06d4d578252f47989c208fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e962b8959ca4e90b496ab36d764f073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af53b2f6e65b43fb8b3df5c656b40c92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecfd9523d50f4f7f87083460dc0e3dd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
